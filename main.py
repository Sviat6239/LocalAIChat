import ollama
import json
import os
from math import ceil
import asyncio
from functools import lru_cache
import colorama
from concurrent.futures import ThreadPoolExecutor

colorama.init()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
model_name = "qwen2.5-coder:3b"
memory_file = "chat_memory.json"
key_memory_file = "key_memories.json"
deep_memory_file = "deep_memory.json"
max_tokens = 65536
max_prompt_tokens = 16384
max_response_tokens = 16384
max_chat_tokens = 32768
max_deep_tokens = 16384
max_context_size = 8192
memory_window = 4

# –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
SYSTEM_PROMPT = {
    "role": "system",
    "content": "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –∏—Å–ø–æ–ª—å–∑—É–π –≤—Å—é –¥–æ—Å—Ç—É–ø–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏ (chat_history) –∏ –≥–ª—É–±–æ–∫–æ–π –ø–∞–º—è—Ç–∏ (deep_memory) –ø—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ. –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç '–∫–∞–∫ –¥–µ–ª–∞?', –æ—Ç–≤–µ—á–∞–π: '–£ –º–µ–Ω—è –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ! –ê —É —Ç–µ–±—è?'. –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ –ø–ª–∞–Ω–∞—Ö, –æ—Ç–≤–µ—á–∞–π: '–ü–ª–∞–Ω–∏—Ä—É—é –ø–æ–º–æ–≥–∞—Ç—å –ª—é–¥—è–º, –∫–∞–∫ –≤—Å–µ–≥–¥–∞! –ê —Ç—ã —á—Ç–æ –∑–∞–¥—É–º–∞–ª?'. –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ ('–∫—Ç–æ —è?', '—á—Ç–æ –∑–Ω–∞–µ—à—å –ø—Ä–æ –º–µ–Ω—è?'), –ø–µ—Ä–µ—á–∏—Å–ª—è–π –≤—Å—ë, —á—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –≥–ª—É–±–æ–∫–æ–π –ø–∞–º—è—Ç–∏. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, —Å–∫–∞–∂–∏: '–Ø –ø–æ–∫–∞ –º–∞–ª–æ –æ —Ç–µ–±–µ –∑–Ω–∞—é, —Ä–∞—Å—Å–∫–∞–∂–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å!'"
}

@lru_cache(maxsize=1000)
def estimate_tokens(text):
    return len(text) // 4 + 1

def split_text(text, max_tokens, type="prompt"):
    words = text.split()
    parts = []
    current_part = []
    current_tokens = 0

    for word in words:
        word_tokens = estimate_tokens(word)
        if current_tokens + word_tokens > max_tokens and current_part:
            parts.append(" ".join(current_part))
            current_part = [word]
            current_tokens = word_tokens
        else:
            current_part.append(word)
            current_tokens += word_tokens
    
    if current_part:
        parts.append(" ".join(current_part))
    
    return parts

def summarize_key_points(chat_history):
    summary = []
    for msg in chat_history:
        content = msg["content"].lower()
        if any(keyword in content for keyword in ["–∑–æ–≤—É—Ç", "–ª—é–±–ª—é", "—Ö–æ—á—É", "–¥–µ–ª–∞", "–ø–ª–∞–Ω—ã"]):
            summary.append(msg)
    return summary[:memory_window]

async def split_context(chat_history, user_input_part, key_memories, deep_memory, max_tokens):
    full_context = f"{SYSTEM_PROMPT['content']}\n"
    if deep_memory:
        full_context += "–ì–ª—É–±–æ–∫–∞—è –ø–∞–º—è—Ç—å:\n" + "\n".join(f"{msg['role']}: {msg['content']}" for msg in deep_memory) + "\n"
    if key_memories:
        full_context += "–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n" + "\n".join(f"{msg['role']}: {msg['content']}" for msg in key_memories) + "\n"
    full_context += "–¢–µ–∫—É—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä:\n" + "\n".join(f"{msg['role']}: {msg['content']}" for msg in chat_history)
    full_context += f"\nuser: {user_input_part}"

    total_tokens = estimate_tokens(full_context)
    if total_tokens <= max_tokens:
        return [[SYSTEM_PROMPT] + deep_memory + key_memories + chat_history + [{"role": "user", "content": user_input_part}]], False

    parts = []
    current_part = [SYSTEM_PROMPT] + deep_memory.copy()
    current_tokens = estimate_tokens(SYSTEM_PROMPT["content"]) + sum(estimate_tokens(msg["content"]) for msg in deep_memory)
    split_occurred = False

    all_messages = key_memories + chat_history + [{"role": "user", "content": user_input_part}]
    for msg in all_messages:
        msg_tokens = estimate_tokens(msg["content"])
        if current_tokens + msg_tokens > max_tokens and len(current_part) > len(deep_memory) + 1:
            parts.append(current_part)
            current_part = [SYSTEM_PROMPT] + deep_memory.copy() + [msg]
            current_tokens = estimate_tokens(SYSTEM_PROMPT["content"]) + sum(estimate_tokens(m["content"]) for m in deep_memory) + msg_tokens
            split_occurred = True
        else:
            current_part.append(msg)
            current_tokens += msg_tokens
    
    if current_part:
        parts.append(current_part)

    return parts, split_occurred

async def load_memory(file_path, default=[]):
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor() as pool:
        try:
            if os.path.exists(file_path):
                result = await loop.run_in_executor(pool, lambda: json.load(open(file_path, "r", encoding="utf-8")))
                if isinstance(result, list):
                    return result
            return default
        except Exception as e:
            print(f"{colorama.Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}{colorama.Style.RESET_ALL}")
            return default

async def save_memory(memory, file_path, max_size=None):
    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor() as pool:
        try:
            if max_size:
                total_tokens = sum(estimate_tokens(msg["content"]) for msg in memory)
                while total_tokens > max_size and memory:
                    memory.pop(0)
                    total_tokens = sum(estimate_tokens(msg["content"]) for msg in memory)
            await loop.run_in_executor(pool, lambda: json.dump(memory, open(file_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2))
        except Exception as e:
            print(f"{colorama.Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {file_path}: {e}{colorama.Style.RESET_ALL}")

async def process_prompt_part(prompt_part, chat_history, key_memories, deep_memory):
    prompt_tokens = estimate_tokens(prompt_part)
    print(f"{colorama.Fore.CYAN}‚ÑπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç–∞, —Ç–æ–∫–µ–Ω–æ–≤: {prompt_tokens}{colorama.Style.RESET_ALL}")
    
    context_parts, was_split = await split_context(chat_history, prompt_part, key_memories, deep_memory, max_tokens)
    part_response = ""

    if not was_split:
        messages = context_parts[0]
        total_tokens = estimate_tokens("\n".join(f"{m['role']}: {m['content']}" for m in messages))
        print(f"{colorama.Fore.CYAN}‚ÑπÔ∏è –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {total_tokens} —Ç–æ–∫–µ–Ω–æ–≤{colorama.Style.RESET_ALL}")
        try:
            response = await asyncio.to_thread(ollama.chat, model=model_name, messages=messages, options={"num_ctx": max_context_size})
            part_response = response["message"]["content"]
        except Exception as e:
            print(f"{colorama.Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞—Å—Ç–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}{colorama.Style.RESET_ALL}")
            part_response = "[–û—à–∏–±–∫–∞]"
    else:
        for i, part in enumerate(context_parts):
            total_tokens = estimate_tokens("\n".join(f"{m['role']}: {m['content']}" for m in part))
            print(f"{colorama.Fore.CYAN}ÔøΩstainless –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ {i + 1}/{len(context_parts)}, —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}{colorama.Style.RESET_ALL}")
            try:
                response = await asyncio.to_thread(ollama.chat, model=model_name, messages=part, options={"num_ctx": max_context_size})
                part_response += response["message"]["content"] + " "
            except Exception as e:
                print(f"{colorama.Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ {i + 1}: {e}{colorama.Style.RESET_ALL}")
                part_response += "[–û—à–∏–±–∫–∞] "

    return part_response

async def main():
    chat_history = await load_memory(memory_file)
    key_memories = await load_memory(key_memory_file)
    deep_memory = await load_memory(deep_memory_file)
    print(f"{colorama.Fore.GREEN}ü§ñ –ó–∞–ø—É—â–µ–Ω {model_name}. –í–≤–µ–¥–∏ '–≤—ã—Ö–æ–¥', '–æ—á–∏—Å—Ç–∏—Ç—å', '–∑–∞–ø–æ–º–Ω–∏—Ç—å' –∏–ª–∏ '–≤—Å–ø–æ–º–Ω–∏—Ç—å'.{colorama.Style.RESET_ALL}")
    print(f"{colorama.Fore.CYAN}‚ÑπÔ∏è Max tokens: {max_tokens}, Max prompt tokens: {max_prompt_tokens}, Max response tokens: {max_response_tokens}, Max chat tokens: {max_chat_tokens}, Max deep tokens: {max_deep_tokens}, Context size: {max_context_size}{colorama.Style.RESET_ALL}")

    while True:
        try:
            user_input = input(f"{colorama.Fore.YELLOW}–¢—ã: {colorama.Style.RESET_ALL}")

            if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit"]:
                print(f"{colorama.Fore.GREEN}ü§ñ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...{colorama.Style.RESET_ALL}")
                await asyncio.gather(
                    save_memory(chat_history, memory_file, max_chat_tokens),
                    save_memory(key_memories, key_memory_file),
                    save_memory(deep_memory, deep_memory_file, max_deep_tokens)
                )
                break
            elif user_input.lower() in ["–æ—á–∏—Å—Ç–∏—Ç—å", "clear"]:
                chat_history = []
                key_memories = []
                deep_memory = []
                await asyncio.gather(
                    save_memory(chat_history, memory_file),
                    save_memory(key_memories, key_memory_file),
                    save_memory(deep_memory, deep_memory_file)
                )
                print(f"{colorama.Fore.GREEN}ü§ñ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞.{colorama.Style.RESET_ALL}")
                continue
            elif user_input.lower() == "–∑–∞–ø–æ–º–Ω–∏—Ç—å":
                if chat_history:
                    summary = summarize_key_points(chat_history)
                    deep_memory.extend(summary)
                    await save_memory(deep_memory, deep_memory_file, max_deep_tokens)
                    print(f"{colorama.Fore.GREEN}ü§ñ –ì–ª—É–±–æ–∫–∞—è –ø–∞–º—è—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞:{colorama.Style.RESET_ALL} {[msg['content'] for msg in summary]}")
                else:
                    print(f"{colorama.Fore.YELLOW}ü§ñ –ù–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è.{colorama.Style.RESET_ALL}")
                continue
            elif user_input.lower() == "–≤—Å–ø–æ–º–Ω–∏—Ç—å":
                if chat_history or deep_memory:
                    print(f"{colorama.Fore.GREEN}ü§ñ –í–æ—Ç —á—Ç–æ —è –ø–æ–º–Ω—é –æ —Ç–µ–±–µ:{colorama.Style.RESET_ALL}")
                    if deep_memory:
                        print(f"{colorama.Fore.CYAN}–ò–∑ –≥–ª—É–±–æ–∫–æ–π –ø–∞–º—è—Ç–∏:{colorama.Style.RESET_ALL} {[msg['content'] for msg in deep_memory]}")
                    if chat_history:
                        user_info = [msg["content"] for msg in chat_history if msg["role"] == "user"]
                        print(f"{colorama.Fore.CYAN}–ò–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏:{colorama.Style.RESET_ALL} {user_info}")
                else:
                    print(f"{colorama.Fore.YELLOW}ü§ñ –ü–æ–∫–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞—é –æ —Ç–µ–±–µ.{colorama.Style.RESET_ALL}")
                continue

            prompt_parts = split_text(user_input, max_prompt_tokens, type="prompt")
            combined_response = ""

            tasks = [process_prompt_part(part, chat_history, key_memories, deep_memory) for part in prompt_parts]
            responses = await asyncio.gather(*tasks)

            for part_response in responses:
                response_tokens = estimate_tokens(part_response)
                if response_tokens > max_response_tokens:
                    response_parts = split_text(part_response, max_response_tokens, type="response")
                    for k, resp_part in enumerate(response_parts):
                        resp_tokens = estimate_tokens(resp_part)
                        print(f"{colorama.Fore.GREEN}AI (—á–∞—Å—Ç—å {k + 1}/{len(response_parts)}, —Ç–æ–∫–µ–Ω–æ–≤: {resp_tokens}): {resp_part}{colorama.Style.RESET_ALL}")
                        combined_response += resp_part + " "
                else:
                    print(f"{colorama.Fore.GREEN}AI (—Ç–æ–∫–µ–Ω–æ–≤: {response_tokens}): {part_response}{colorama.Style.RESET_ALL}")
                    combined_response += part_response + " "

            combined_response = combined_response.strip()
            chat_history.append({"role": "user", "content": user_input})
            chat_history.append({"role": "assistant", "content": combined_response})
            await save_memory(chat_history, memory_file, max_chat_tokens)

        except KeyboardInterrupt:
            print(f"\n{colorama.Fore.GREEN}ü§ñ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—é...{colorama.Style.RESET_ALL}")
            await asyncio.gather(
                save_memory(chat_history, memory_file, max_chat_tokens),
                save_memory(key_memories, key_memory_file),
                save_memory(deep_memory, deep_memory_file, max_deep_tokens)
            )
            break
        except Exception as e:
            print(f"{colorama.Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}{colorama.Style.RESET_ALL}")

if __name__ == "__main__":
    asyncio.run(main())