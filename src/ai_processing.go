package ai_processing

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"sync"
	"time"

	"yourproject/config"    // –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å
	"yourproject/display"  // –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å
	"yourproject/logging_setup"
	"yourproject/memory"
	"yourproject/utils"
)

type Message struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

func ProcessCommandWithAI(userInput string, chatHistory []Message) string {
	// –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–æ–º–∞–Ω–¥—ã
	messages := []Message{
		{Role: "system", Content: "You are a command interpreter. Identify if the input is a command (e.g., exit, clear, remember, recall, key, moment, messages, –∑–∞–ø–æ–º–Ω–∏—Ç—å, –≤—Å–ø–æ–º–Ω–∏—Ç—å, –æ—á–∏—Å—Ç–∏—Ç—å) and return it in the format: /command [args]. Return None if not a command."},
		{Role: "user", Content: fmt.Sprintf("Interpret this input: '%s'", userInput)},
	}

	// –í—ã–∑–æ–≤ Ollama —á–µ—Ä–µ–∑ HTTP (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å —ç–º—É–ª–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ –≥–æ—Ä—É—Ç–∏–Ω—ã)
	resp, err := callOllama(messages, config.MODEL_NAME, config.MAX_CONTEXT_SIZE)
	if err != nil {
		logging_setup.LogQueue <- fmt.Sprintf("\033[31m‚ö†Ô∏è Error interpreting command: %v\033[0m", err)
		return ""
	}

	interpretedCommand := strings.TrimSpace(resp)
	if strings.HasPrefix(interpretedCommand, "/") {
		return interpretedCommand
	}
	return ""
}

func SummarizeUserInfo(deepMemory, keyMemories, chatHistory []Message) string {
	// –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø–∞–º—è—Ç–∏
	deepInfo := make([]string, 0)
	for _, msg := range deepMemory {
		if msg.Role == "user" {
			deepInfo = append(deepInfo, msg.Content)
		}
	}
	keyInfo := make([]string, 0)
	for _, msg := range keyMemories {
		if msg.Role == "user" {
			keyInfo = append(keyInfo, msg.Content)
		}
	}
	chatInfo := make([]string, 0)
	for _, msg := range chatHistory {
		if msg.Role == "user" {
			chatInfo = append(chatInfo, msg.Content)
		}
	}

	allInfo := memory.FilterRedundantInfo(append(append(deepInfo, keyInfo...), chatInfo...))
	if len(allInfo) == 0 {
		return "I have limited information about you so far. Please share more about yourself."
	}

	summaryPrompt := fmt.Sprintf(
		"Create a concise and formal summary about the user based on the following data: %s. Limit the summary to 1-2 sentences, exclude repetitive or minor details, focus on key facts, and present the information in a coherent manner.",
		strings.Join(allInfo, ", "),
	)
	messages := []Message{
		{
			Role: "system",
			Content: "You are an assistant specialized in creating concise and accurate summaries. Respond formally, avoid slang, combine facts into a cohesive description, and keep it to 1-2 sentences.",
		},
		{Role: "user", Content: summaryPrompt},
	}

	resp, err := callOllama(messages, config.MODEL_NAME, config.MAX_CONTEXT_SIZE)
	if err != nil {
		logging_setup.LogQueue <- fmt.Sprint("\033[31m‚ö†Ô∏è Error summarizing user info: %v\033[0m", err)
		return "I have information about you, but I cannot summarize it right now."
	}
	return strings.TrimSpace(resp)
}

func ProcessPromptPart(promptPart string, chatHistory, keyMemories, deepMemory, compiledMemory []Message) (string, error) {
	promptTokens := utils.EstimateTokens(promptPart)
	logging_setup.LogQueue <- fmt.Sprintf("‚ÑπÔ∏è Processing prompt part, tokens: %d", promptTokens)

	// –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
	contextParts, wasSplit, err := memory.SplitContext(chatHistory, promptPart, keyMemories, deepMemory, compiledMemory, config.MAX_TOKENS)
	if err != nil {
		return "", err
	}
	partResponse := ""

	if !wasSplit {
		messages := contextParts[0]
		totalTokens := utils.EstimateTokens(strings.Join(flattenMessages(messages), "\n"))
		logging_setup.LogQueue <- fmt.Sprintf("‚ÑπÔ∏è Total context: %d tokens", totalTokens)

		// –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–º–∞–Ω–¥—ã "remember" –∏–ª–∏ "–∑–∞–ø–æ–º–Ω–∏"
		promptLower := strings.ToLower(promptPart)
		if strings.Contains(promptLower, "–∑–∞–ø–æ–º–Ω–∏") || strings.Contains(promptLower, "remember") {
			if len(chatHistory) >= 2 && strings.Contains(promptLower, "—Å–≤–æ–∏ —Å–ª–æ–≤–∞") {
				lastResponse := ""
				if chatHistory[len(chatHistory)-2].Role == "assistant" {
					lastResponse = chatHistory[len(chatHistory)-2].Content
				}
				if lastResponse != "" {
					keyMemories = append(keyMemories, Message{Role: "assistant", Content: lastResponse})
					memory.SaveMemory(keyMemories, "key_memories", 0)
					logging_setup.LogQueue <- fmt.Sprintf("ü§ñ Saved my last response to key memory: %s", lastResponse)
					return "OK", nil
				}
			} else {
				keyMemories = append(keyMemories, Message{Role: "user", Content: promptPart})
				memory.SaveMemory(keyMemories, "key_memories", 0)
				logging_setup.LogQueue <- fmt.Sprintf("ü§ñ Saved to key memory: %s", promptPart)
				return "OK", nil
			}
		}

		// –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –æ –ø–∞–º—è—Ç–∏
		keywords := []string{"who am i", "what do you know", "what do you remember", "—á—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å", "–∫—Ç–æ —è"}
		for _, keyword := range keywords {
			if strings.Contains(promptLower, keyword) {
				summary := SummarizeUserInfo(deepMemory, keyMemories, chatHistory)
				response := fmt.Sprintf("Here is what I know about you: %s", summary)
				display.TypeText(response, "\033[32m", config.TOKEN_DISPLAY_DELAY)
				return response, nil
			}
		}

		// –í—ã–∑–æ–≤ Ollama —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –æ—Ç–≤–µ—Ç–æ–º
		payload := map[string]interface{}{
			"model":    config.MODEL_NAME,
			"messages": messages,
			"stream":   true,
			"options": map[string]int{
				"num_ctx": config.MAX_CONTEXT_SIZE,
			},
		}
		resp, err := streamOllama(payload)
		if err != nil {
			logging_setup.LogQueue <- "‚ö†Ô∏è Error: HTTP error"
			return "[Error]", err
		}
		return strings.TrimSpace(resp), nil
	} else {
		var wg sync.WaitGroup
		responses := make(chan string, len(contextParts))
		for i, part := range contextParts {
			wg.Add(1)
			go func(i int, part []Message) {
				defer wg.Done()
				totalTokens := utils.EstimateTokens(strings.Join(flattenMessages(part), "\n"))
				logging_setup.LogQueue <- fmt.Sprintf("‚ÑπÔ∏è Processing context part %d/%d, tokens: %d", i+1, len(contextParts), totalTokens)

				payload := map[string]interface{}{
					"model":    config.MODEL_NAME,
					"messages": part,
					"stream":   true,
					"options": map[string]int{
						"num_ctx": config.MAX_CONTEXT_SIZE,
					},
				}
				resp, err := streamOllama(payload)
				if err != nil {
					logging_setup.LogQueue <- fmt.Sprintf("‚ö†Ô∏è Error: HTTP error for part %d", i+1)
					responses <- "[Error] "
					return
				}
				responses <- strings.TrimSpace(resp) + " "
			}(i, part)
		}

		go func() {
			wg.Wait()
			close(responses)
		}()

		for resp := range responses {
			partResponse += resp
		}
		return strings.TrimSpace(partResponse), nil
	}
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ Ollama —á–µ—Ä–µ–∑ HTTP
func callOllama(messages []Message, model string, contextSize int) (string, error) {
	payload := map[string]interface{}{
		"model":    model,
		"messages": messages,
		"stream":   false,
		"options": map[string]int{
			"num_ctx": contextSize,
		},
	}
	jsonData, _ := json.Marshal(payload)

	resp, err := http.Post(config.OLLAMA_URL, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", err
	}

	var result map[string]interface{}
	if err := json.Unmarshal(body, &result); err != nil {
		return "", err
	}

	if message, ok := result["message"].(map[string]interface{}); ok {
		if content, ok := message["content"].(string); ok {
			return content, nil
		}
	}
	return "", fmt.Errorf("invalid response format")
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–∑–æ–≤–∞ Ollama (—ç–º—É–ª—è—Ü–∏—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞)
func streamOllama(payload map[string]interface{}) (string, error) {
	jsonData, _ := json.Marshal(payload)
	resp, err := http.Post(config.OLLAMA_URL, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	responseText := ""
	inCodeBlock := false
	lastTokenWasSpace := false

	decoder := json.NewDecoder(resp.Body)
	for {
		var chunk map[string]interface{}
		if err := decoder.Decode(&chunk); err == io.EOF {
			break
		} else if err != nil {
			continue
		}

		if message, ok := chunk["message"].(map[string]interface{}); ok {
			if content, ok := message["content"].(string); ok {
				token := content
				responseText += token
				lastTokenWasSpace = displayToken(token, inCodeBlock, lastTokenWasSpace)
				inCodeBlock = lastTokenWasSpace // –ü—Ä–æ—Å—Ç–∞—è —ç–º—É–ª—è—Ü–∏—è –±–ª–æ–∫–∞ –∫–æ–¥–∞
				time.Sleep(time.Duration(config.TOKEN_DISPLAY_DELAY * 1000 * 1000 * 1000)) // –ó–∞–¥–µ—Ä–∂–∫–∞ –≤ –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥–∞—Ö
			}
		}
	}
	return responseText, nil
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ (—ç–º—É–ª—è—Ü–∏—è display_token)
func displayToken(token string, inCodeBlock bool, lastTokenWasSpace bool) bool {
	// –ü—Ä–æ—Å—Ç–∞—è —ç–º—É–ª—è—Ü–∏—è: –≤—ã–≤–æ–¥–∏–º —Ç–æ–∫–µ–Ω –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª —Å –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π
	fmt.Print(token)
	if strings.HasPrefix(token, "```") || strings.HasSuffix(token, "```") {
		return !lastTokenWasSpace
	}
	return strings.TrimSpace(token) == ""
}

// –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Å—Ç—Ä–æ–∫—É
func flattenMessages(messages []Message) []string {
	result := make([]string, 0, len(messages))
	for _, msg := range messages {
		result = append(result, fmt.Sprintf("%s: %s", msg.Role, msg.Content))
	}
	return result
}